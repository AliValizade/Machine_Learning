{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rHfHcd7HaNan"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_digits()"
      ],
      "metadata": {
        "id": "zcGrrwWUbcga"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe8N06oSjAmb",
        "outputId": "70346da5-8027-4dac-c1e0-370a4e575cc9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.data[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukw1WeZOpink",
        "outputId": "40d891e1-7dde-4690-bac0-79ab367488d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.,  0.,  0.,  2., 13.,  0.,  0.,  0.,  0.,  0.,  0.,  8., 15.,\n",
              "        0.,  0.,  0.,  0.,  0.,  5., 16.,  5.,  2.,  0.,  0.,  0.,  0.,\n",
              "       15., 12.,  1., 16.,  4.,  0.,  0.,  4., 16.,  2.,  9., 16.,  8.,\n",
              "        0.,  0.,  0., 10., 14., 16., 16.,  4.,  0.,  0.,  0.,  0.,  0.,\n",
              "       13.,  8.,  0.,  0.,  0.,  0.,  0.,  0., 13.,  6.,  0.,  0.])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCis7EKAjLXi",
        "outputId": "1c6bf130-ccfd-4595-8396-c8c6cd515baf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 8, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.images[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmPWNiJ7poEJ",
        "outputId": "f62cb802-c052-4fb3-b2a1-05beb1527948"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  0.,  0.,  2., 13.,  0.,  0.,  0.],\n",
              "       [ 0.,  0.,  0.,  8., 15.,  0.,  0.,  0.],\n",
              "       [ 0.,  0.,  5., 16.,  5.,  2.,  0.,  0.],\n",
              "       [ 0.,  0., 15., 12.,  1., 16.,  4.,  0.],\n",
              "       [ 0.,  4., 16.,  2.,  9., 16.,  8.,  0.],\n",
              "       [ 0.,  0., 10., 14., 16., 16.,  4.,  0.],\n",
              "       [ 0.,  0.,  0.,  0., 13.,  8.,  0.,  0.],\n",
              "       [ 0.,  0.,  0.,  0., 13.,  6.,  0.,  0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT1loo-djblk",
        "outputId": "b618d96d-9874-49cf-e980-2a88f9f698f6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797,)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.target[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNPzfWqqptZq",
        "outputId": "3443d56e-4ab0-4669-bfb1-d06e36c17a80"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(dataset.images[100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "8rLA1yZOjxLF",
        "outputId": "55f08871-18bd-4db1-d86f-d8aa8f06c640"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0f85a6cc10>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = dataset.data\n",
        "Y = dataset.target\n",
        "Y = np.eye(10)[Y]  # One hot\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)"
      ],
      "metadata": {
        "id": "qW6IuVODtQsl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "051ybYXAtzWY",
        "outputId": "0b6376ab-1dfa-4dca-b212-7b0b6aef54d1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1437, 64), (360, 64), (1437, 10), (360, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(X):\n",
        "  return 1 / (1 + np.exp(-X))\n",
        "\n",
        "def softmax(X):\n",
        "  return np.exp(X) / np.sum(np.exp(X))\n",
        "\n",
        "def root_mean_squired_error(Y_gt, Y_pred):\n",
        "  return np.sqrt(np.mean((Y_gt - Y_pred) ** 2))"
      ],
      "metadata": {
        "id": "i_PS2bzzt8yg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 80\n",
        "lr = 0.001\n",
        "D_in = X_train.shape[1]\n",
        "H1 = 128\n",
        "H2 = 32\n",
        "D_out = Y_train.shape[1]"
      ],
      "metadata": {
        "id": "qN-N16fDvG2h"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = np.random.randn(D_in, H1)\n",
        "W2 = np.random.randn(H1, H2)\n",
        "W3 = np.random.randn(H2, D_out)\n",
        "\n",
        "B1 = np.random.randn(1, H1)\n",
        "B2 = np.random.randn(1, H2)\n",
        "B3 = np.random.randn(1, D_out)"
      ],
      "metadata": {
        "id": "cxYVOTWexP6O"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  # Train\n",
        "  Y_pred_train = []\n",
        "  for x, y in zip(X_train, Y_train):\n",
        "    x = x.reshape(-1, 1)\n",
        "\n",
        "    # >>>>> forward >>>>>\n",
        "    # layer 1\n",
        "    out1 = sigmoid(x.T @ W1 + B1)\n",
        "    # layer 2\n",
        "    out2 = sigmoid(out1 @ W2 + B2)\n",
        "    # layer 3\n",
        "    out3 = softmax(out2 @ W3 + B3)\n",
        "    y_pred = out3\n",
        "\n",
        "    Y_pred_train.append(y_pred)\n",
        "\n",
        "    loss = root_mean_squired_error(y, Y_pred_train)\n",
        "\n",
        "    # <<<<<< backward <<<<<<\n",
        "    # layer 3\n",
        "    error = -2 * (y - y_pred)\n",
        "    grad_B3 = error\n",
        "    grad_W3 = out2.T @ error\n",
        "    # layer 2\n",
        "    error = error @ W3.T * out2 * (1 - out2)\n",
        "    grad_B2 = error\n",
        "    grad_W2 = out1.T @ error\n",
        "    # layer 1\n",
        "    error = error @ W2.T * out1 * (1 - out1)\n",
        "    grad_B1 = error\n",
        "    grad_W1 = x @ error\n",
        "\n",
        "\n",
        "    # ===== update =====\n",
        "    # layer 1\n",
        "    W1 -= lr * grad_W1\n",
        "    B1 -= lr * grad_B1\n",
        "    # layer 2\n",
        "    W2 -= lr * grad_W2\n",
        "    B2 -= lr * grad_B2\n",
        "    # layer 3\n",
        "    W3 -= lr * grad_W3\n",
        "    B3 -= lr *grad_B3\n",
        "\n",
        "  # Test\n",
        "  Y_pred_test = []\n",
        "  for x, y in zip(X_test, Y_test):\n",
        "    x = x.reshape(-1, 1)\n",
        "\n",
        "    # >>>>> forward >>>>>\n",
        "    # layer 1\n",
        "    out1 = sigmoid(x.T @ W1 + B1)\n",
        "    # layer 2\n",
        "    out2 = sigmoid(out1 @ W2 + B2)\n",
        "    # layer 3\n",
        "    out3 = softmax(out2 @ W3 + B3)\n",
        "    y_pred = out3\n",
        "\n",
        "    Y_pred_test.append(y_pred)\n",
        "\n",
        "    loss = root_mean_squired_error(y, Y_pred_test)\n",
        "\n",
        "  Y_pred_train = np.array(Y_pred_train).reshape(-1, 10)\n",
        "  loss_train = root_mean_squired_error(Y_train, Y_pred_train)\n",
        "  accuracy_train = np.sum(np.argmax(Y_train, axis=1) == np.argmax(Y_pred_train, axis=1)) / len(Y_train)\n",
        "  print('Train ==> ', ' ❌', loss_train, '\\t✅', accuracy_train)\n",
        "\n",
        "  Y_pred_test = np.array(Y_pred_test).reshape(-1, 10)\n",
        "  loss_test = root_mean_squired_error(Y_test, Y_pred_test)\n",
        "  accuracy_test = np.sum(np.argmax(Y_test, axis=1) == np.argmax(Y_pred_test, axis=1)) / len(Y_test)\n",
        "  print('Test ==> ', ' ❌', loss_test, '\\t✅', accuracy_test)\n",
        "  print('='*65)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg2biRHnjVu3",
        "outputId": "83aaacd5-f8d5-47c2-d577-89e53e4af5f2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train ==>   ❌ 0.3142159196054691 \t✅ 0.19485038274182323\n",
            "Test ==>   ❌ 0.29278489484982395 \t✅ 0.26666666666666666\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.2791018655394333 \t✅ 0.34585942936673625\n",
            "Test ==>   ❌ 0.2748776149944767 \t✅ 0.38333333333333336\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.26219715522692294 \t✅ 0.46346555323590816\n",
            "Test ==>   ❌ 0.2596749244374446 \t✅ 0.48333333333333334\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.24781647066573653 \t✅ 0.546276965901183\n",
            "Test ==>   ❌ 0.24840022553007785 \t✅ 0.5361111111111111\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.23466325439981464 \t✅ 0.6263048016701461\n",
            "Test ==>   ❌ 0.23671579595186173 \t✅ 0.5888888888888889\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.223064106181715 \t✅ 0.6701461377870563\n",
            "Test ==>   ❌ 0.22730004636989787 \t✅ 0.6277777777777778\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.21292496523456259 \t✅ 0.7118997912317327\n",
            "Test ==>   ❌ 0.21901312690925956 \t✅ 0.6611111111111111\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.20281615992749064 \t✅ 0.7411273486430062\n",
            "Test ==>   ❌ 0.2120108813688005 \t✅ 0.6888888888888889\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.1937607601198623 \t✅ 0.7703549060542797\n",
            "Test ==>   ❌ 0.20587656485059194 \t✅ 0.7083333333333334\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.18550585986049187 \t✅ 0.7947112038970077\n",
            "Test ==>   ❌ 0.20019720738730645 \t✅ 0.7166666666666667\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.17820490178807571 \t✅ 0.8086290883785665\n",
            "Test ==>   ❌ 0.19463147186939536 \t✅ 0.7472222222222222\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.1716142549044339 \t✅ 0.8211551844119693\n",
            "Test ==>   ❌ 0.1894161196669082 \t✅ 0.7722222222222223\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.16548890645736017 \t✅ 0.837160751565762\n",
            "Test ==>   ❌ 0.18492547488464262 \t✅ 0.775\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.1599243212691721 \t✅ 0.8503827418232429\n",
            "Test ==>   ❌ 0.1809523824841332 \t✅ 0.7888888888888889\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.15491843458875665 \t✅ 0.8656924147529576\n",
            "Test ==>   ❌ 0.17752701746658445 \t✅ 0.7972222222222223\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.15044042006849936 \t✅ 0.8761308281141267\n",
            "Test ==>   ❌ 0.17436648303532945 \t✅ 0.8055555555555556\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.1464354090516719 \t✅ 0.8823938761308281\n",
            "Test ==>   ❌ 0.17160435485527287 \t✅ 0.8138888888888889\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.14271158058089445 \t✅ 0.8872651356993737\n",
            "Test ==>   ❌ 0.16927073540988388 \t✅ 0.8222222222222222\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.13919126787002856 \t✅ 0.8921363952679193\n",
            "Test ==>   ❌ 0.16720674654781775 \t✅ 0.825\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.1359060642874564 \t✅ 0.8970076548364648\n",
            "Test ==>   ❌ 0.16526602223176012 \t✅ 0.825\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.1328755962669073 \t✅ 0.9011830201809325\n",
            "Test ==>   ❌ 0.16335196343342687 \t✅ 0.8277777777777777\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.13006429417734994 \t✅ 0.9032707028531664\n",
            "Test ==>   ❌ 0.16149630354055133 \t✅ 0.8277777777777777\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.12743318888041266 \t✅ 0.9081419624217119\n",
            "Test ==>   ❌ 0.15975476058478294 \t✅ 0.8333333333333334\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.12490860105029489 \t✅ 0.9123173277661796\n",
            "Test ==>   ❌ 0.15813408747565397 \t✅ 0.8388888888888889\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.12248483257434867 \t✅ 0.9164926931106472\n",
            "Test ==>   ❌ 0.1566957988535652 \t✅ 0.8361111111111111\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.12021151450614259 \t✅ 0.919276270006959\n",
            "Test ==>   ❌ 0.1554751162157296 \t✅ 0.8416666666666667\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.11800896770932537 \t✅ 0.9213639526791928\n",
            "Test ==>   ❌ 0.15423129617586034 \t✅ 0.85\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.1158891690762118 \t✅ 0.9241475295755045\n",
            "Test ==>   ❌ 0.15299711813807426 \t✅ 0.85\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.1138680506054201 \t✅ 0.9269311064718163\n",
            "Test ==>   ❌ 0.15175377413357108 \t✅ 0.85\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.11191387095798325 \t✅ 0.9276270006958942\n",
            "Test ==>   ❌ 0.1505029327979747 \t✅ 0.8527777777777777\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.10999691374436053 \t✅ 0.9290187891440501\n",
            "Test ==>   ❌ 0.14929096124268446 \t✅ 0.8527777777777777\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.1080803183118078 \t✅ 0.9311064718162839\n",
            "Test ==>   ❌ 0.14821318226000788 \t✅ 0.8611111111111112\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.10621698373642477 \t✅ 0.9331941544885177\n",
            "Test ==>   ❌ 0.14725092627916755 \t✅ 0.8611111111111112\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.10443111215047872 \t✅ 0.9366736256089074\n",
            "Test ==>   ❌ 0.1463209177177333 \t✅ 0.8583333333333333\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.10274102531336946 \t✅ 0.9394572025052192\n",
            "Test ==>   ❌ 0.14540418120061016 \t✅ 0.8583333333333333\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.10114263761821542 \t✅ 0.942240779401531\n",
            "Test ==>   ❌ 0.1445487449525476 \t✅ 0.8583333333333333\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.0996306898103477 \t✅ 0.942936673625609\n",
            "Test ==>   ❌ 0.14377166557483334 \t✅ 0.8611111111111112\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.0982090827754059 \t✅ 0.9443284620737648\n",
            "Test ==>   ❌ 0.1430335738378257 \t✅ 0.8583333333333333\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.09684672336290298 \t✅ 0.9457202505219207\n",
            "Test ==>   ❌ 0.14228748342347727 \t✅ 0.8583333333333333\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.09550326976967333 \t✅ 0.9478079331941545\n",
            "Test ==>   ❌ 0.14149486310032855 \t✅ 0.8611111111111112\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.09414090933618656 \t✅ 0.9512874043145442\n",
            "Test ==>   ❌ 0.14065794965221315 \t✅ 0.8583333333333333\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.09276558409204029 \t✅ 0.9526791927627001\n",
            "Test ==>   ❌ 0.1399069253683261 \t✅ 0.8638888888888889\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.09141646563250663 \t✅ 0.9554627696590118\n",
            "Test ==>   ❌ 0.13934197336206572 \t✅ 0.8638888888888889\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.0901369325688499 \t✅ 0.9589422407794015\n",
            "Test ==>   ❌ 0.1388542707537406 \t✅ 0.8638888888888889\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.0889394610221529 \t✅ 0.9596381350034795\n",
            "Test ==>   ❌ 0.13837054320664702 \t✅ 0.8638888888888889\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.08778998226860707 \t✅ 0.9603340292275574\n",
            "Test ==>   ❌ 0.1378509727326191 \t✅ 0.8666666666666667\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.08664919121634493 \t✅ 0.9617258176757133\n",
            "Test ==>   ❌ 0.1372267603283318 \t✅ 0.8694444444444445\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.08545020893813593 \t✅ 0.9624217118997912\n",
            "Test ==>   ❌ 0.1365961221939524 \t✅ 0.875\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.08415761019062155 \t✅ 0.9638135003479471\n",
            "Test ==>   ❌ 0.13621035138601595 \t✅ 0.875\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.08303583676029085 \t✅ 0.9638135003479471\n",
            "Test ==>   ❌ 0.1357836078303909 \t✅ 0.875\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.08200725825741147 \t✅ 0.965205288796103\n",
            "Test ==>   ❌ 0.13535260455736908 \t✅ 0.875\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.08103935820834887 \t✅ 0.965205288796103\n",
            "Test ==>   ❌ 0.13491889952902628 \t✅ 0.875\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.08011911960850523 \t✅ 0.9659011830201809\n",
            "Test ==>   ❌ 0.13449106081981604 \t✅ 0.875\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.07923185890069322 \t✅ 0.9679888656924147\n",
            "Test ==>   ❌ 0.13407515242793658 \t✅ 0.875\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.07836706806548181 \t✅ 0.9686847599164927\n",
            "Test ==>   ❌ 0.13366843852348542 \t✅ 0.875\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.07751928839779093 \t✅ 0.9693806541405706\n",
            "Test ==>   ❌ 0.13325821407119492 \t✅ 0.8805555555555555\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.07668164964405784 \t✅ 0.9714683368128044\n",
            "Test ==>   ❌ 0.13283598030197996 \t✅ 0.8805555555555555\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.07584046789533265 \t✅ 0.9721642310368824\n",
            "Test ==>   ❌ 0.13239868978968464 \t✅ 0.8805555555555555\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.0749886238874479 \t✅ 0.9728601252609603\n",
            "Test ==>   ❌ 0.13194635959924897 \t✅ 0.8805555555555555\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.07414262338272753 \t✅ 0.9742519137091162\n",
            "Test ==>   ❌ 0.13149167644979862 \t✅ 0.8805555555555555\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.07330884091336712 \t✅ 0.9749478079331941\n",
            "Test ==>   ❌ 0.13105289751972332 \t✅ 0.8833333333333333\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.07246066307243794 \t✅ 0.9770354906054279\n",
            "Test ==>   ❌ 0.1306498366720818 \t✅ 0.8833333333333333\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.07161657186431156 \t✅ 0.9791231732776617\n",
            "Test ==>   ❌ 0.1302977268537246 \t✅ 0.8861111111111111\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.0708227810610659 \t✅ 0.9798190675017397\n",
            "Test ==>   ❌ 0.12997354500150546 \t✅ 0.8888888888888888\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.07004901049488388 \t✅ 0.9798190675017397\n",
            "Test ==>   ❌ 0.12965597813557841 \t✅ 0.8861111111111111\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.06928143792101395 \t✅ 0.9798190675017397\n",
            "Test ==>   ❌ 0.12934692756621324 \t✅ 0.8888888888888888\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.06852486403770887 \t✅ 0.9805149617258176\n",
            "Test ==>   ❌ 0.12906410587185105 \t✅ 0.8888888888888888\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.06779444751587083 \t✅ 0.9812108559498957\n",
            "Test ==>   ❌ 0.12880627997550417 \t✅ 0.8888888888888888\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.06708838572813687 \t✅ 0.9812108559498957\n",
            "Test ==>   ❌ 0.12856203131158284 \t✅ 0.8916666666666667\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.06640342539533428 \t✅ 0.9812108559498957\n",
            "Test ==>   ❌ 0.12832317981565497 \t✅ 0.8916666666666667\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.06572964797264072 \t✅ 0.9812108559498957\n",
            "Test ==>   ❌ 0.12808993658458934 \t✅ 0.8888888888888888\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.06505406913595979 \t✅ 0.9819067501739736\n",
            "Test ==>   ❌ 0.12786332224340427 \t✅ 0.8916666666666667\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.06438139724014617 \t✅ 0.9839944328462074\n",
            "Test ==>   ❌ 0.12763719154833855 \t✅ 0.8916666666666667\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.06372911957615847 \t✅ 0.9846903270702854\n",
            "Test ==>   ❌ 0.127420232276213 \t✅ 0.8916666666666667\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.06308263945315641 \t✅ 0.9846903270702854\n",
            "Test ==>   ❌ 0.12723092159328672 \t✅ 0.8916666666666667\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.06241896641494441 \t✅ 0.9846903270702854\n",
            "Test ==>   ❌ 0.12707973927980892 \t✅ 0.8916666666666667\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.061763634759267036 \t✅ 0.9846903270702854\n",
            "Test ==>   ❌ 0.12696856370256845 \t✅ 0.8916666666666667\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.061138018113559435 \t✅ 0.9846903270702854\n",
            "Test ==>   ❌ 0.1269057678920698 \t✅ 0.8916666666666667\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.06051785293806578 \t✅ 0.9853862212943633\n",
            "Test ==>   ❌ 0.12687422272092952 \t✅ 0.8944444444444445\n",
            "=================================================================\n",
            "Train ==>   ❌ 0.05988525879118947 \t✅ 0.9860821155184412\n",
            "Test ==>   ❌ 0.12675376130063182 \t✅ 0.8944444444444445\n",
            "=================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "MJtRasxTkB_W"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('4.jpg')\n",
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPNiK7t3hnSR",
        "outputId": "6ac63191-2764-4409-9009-02b2899d7c6c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 8, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uQ4Tg76iCUa",
        "outputId": "830b77a1-b4e6-46ff-df9d-acef441ad882"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = image.reshape(64, 1)\n",
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXcZkexWihdd",
        "outputId": "3b988dd1-fbc0-4c17-bf6b-79c22efdb91e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = image\n",
        "\n",
        "# >>>>> forward >>>>>\n",
        "# layer 1\n",
        "out1 = sigmoid(x.T @ W1 + B1)\n",
        "# layer 2\n",
        "out2 = sigmoid(out1 @ W2 + B2)\n",
        "# layer 3\n",
        "out3 = softmax(out2 @ W3 + B3)\n",
        "\n",
        "y_pred = out3\n",
        "\n",
        "print(np.argmax(y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWAcZzMvisoU",
        "outputId": "54923525-8d1f-47de-fded-6bf9c9976af2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-6c9747543e2e>:2: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-X))\n"
          ]
        }
      ]
    }
  ]
}